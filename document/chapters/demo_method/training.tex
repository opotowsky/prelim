\subsection{Spent Nuclear Fuel Simulations}
\label{sec:snfsim}

Because creating databases from real measurements to represent reactor
technologies from around the world is impossible, the database in this study is
created from high-fidelity simulations via \gls{ORIGEN} \cite{origen}.  A set
of simulations of \gls{SNF} at different burnups and cooling times comprises
the database.  Of interest to an entity trying to create a weapon is partially
irradiated fuel if they have plutonium separations capabilities or any
radioactive substance in the case of a dirty bomb.  Addressing the former, a
smaller burnup than is typical for \gls{SNF} from a commercial reactor is used
in the previous work. This will be repeated here for demonstrative purposes,
but expanded upon in future work.

\begin{table}[!hp]
  \centering
  \begin{subtable}{\linewidth}
    \centering
    \includegraphics[height=0.7\textheight]{./chapters/demo_method/TrainData.png}
    \caption{Reactor types and uranium-235 enrichment [weight\%]}
    \label{tbl:rxtrtype}
    \vspace*{5mm}
  \end{subtable}
  \begin{subtable}{\linewidth}
    \centering
    \includegraphics[width=0.7\linewidth]{./chapters/demo_method/TrainData2.png}
    \caption{Simulation space defining reactor parameters and cooling time}
    \label{tbl:rxtrparam}
  \end{subtable}%
  \caption{Design of the Training Set Space}
  \label{tbl:train}
\end{table}

As mentioned in Section \ref{sec:optvalid}, many algorithms are developed on
the assumption that the training set will be \acrfull{i.i.d.}. This is
important so that the model does not overvalue or overfit a certain area in the
training space. As this is not easily proven, it requires extensive knowledge
of worldwide commercial reactors. Most obviously, a truly \gls{i.i.d.} training
set would go beyond the lower burnups, but this is purely for demonstration
with a single use case in mind.  

The training database is thus constructed by simulating the same training set
space as described in Ref.  \cite{dayman_feasibility_2013}, shown in Table
\ref{tbl:train}. For each entry shown here the simulations included the span of
cooling times and burnups in Table \ref{tbl:rxtrparam}. The power densities,
listed enrichments, and moderator densities were kept constant. The burnup for
the \glspl{PWR} was increased in steps of $950\ MWd/MTU$, resulting in 19
measured time points. The \glspl{BWR} and \glspl{PHWR} had 18 measurements with
burnup steps of $690\ MWd/MTU$.  This resulted in 2313 observations in the
training set.

Feature reduction on the training set is manual in nature: the top 200 nuclides
by concentration in each observation were retained. This resulted in 303
features in total in the training set. All nuclides below the top 200
concentrations in each observation were set to zero. Other reduction techniques
using domain knowledge (e.g., filtering for actinides) and statistical
dimensionality reduction (e.g., \gls{PCA}) are not addressed in this
demonstration.

\begin{table}[!hbt]
  \centering
  \includegraphics[width=0.95\linewidth]{./chapters/demo_method/TestData.png}
  \caption{Design of the Testing Set Space}
  \label{tbl:test}
\end{table}

While typical \gls{ML} studies choose the testing set randomly from the
training set, the previous work used an external one, shown in Table
\ref{tbl:test}.  The simulations retained the default values from the training
set; only the values shown here were changed. This provided 92 observations for
testing.  The test set was designed to have values in between the trained
values of burnup and cooling times for the same set of reactors.  However, a
systematically chosen test set can be problematic since it may not cover parts
of the training space.  It is implemented in this study for comparison, but
\gls{CV} will be used moving forward. More specifically, using \textit{k}-fold
\gls{CV} is expected to better indicate the model performance. 

\subsection{Information Reduction}
\label{sec:inforeduc}

Since the overall goal of this project is to determine how much information to
what quality is needed to train an \gls{ML} model, there will be an
information reduction manipulation applied to the training data set. This study
evaluates the impact of randomly introduced error on the ability of the
algorithms to correctly predict the burnup. 

The three algorithms will be evaluated with error applied to each nuclide
vector in the training set.  A maximum error ranging from $0 - 10\%$ is chosen
for each round of training, and a random error within the range of $[1-E_{max},
1+E_{max}]$ is applied to each component of the nuclide vector.

However, error in a nuclide vector is not random, in fact it is systematic and
dependent on a number of known sources of uncertainty. The next study will
introduce error by limiting the nuclides to only those that can be measured
with a gamma spectrometer. Although this is initially done using the
availability of gamma energies in \gls{ORIGEN}, \gls{GADRAS} can provide more
\gls{DRF}s to further reduce information given to the algorithm.


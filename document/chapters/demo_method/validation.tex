\subsection{Model Diagnostics}
\label{sec:algeval}

Machine learning algorithms are heavily dependent on the inputs and parameters
given to them, such as training set sizes, regularization, learning rates, etc.
From the results shown in Section \ref{sec:statmodel}, it is clear there is
room for improvement.  The diagnostic plots show the errors of the predicted
burnup values to the actual burnup values with respect to some variable on the
\textit{x}-axis.  As previously introduced in Section \ref{sec:optvalid}, the
errors are compared to the training error to understand the generalization
strength with respect to training set size (learning curves) and the algorithm
parameters governing model complexity (validation curves). 

In addition to machine learning best practices, another layer of comparison is
added here.  Because it is difficult to ensure consistently representative
testing data, the accuracy of a learned model should not depend on only one
testing set.  The learned model's accuracy is better estimated by using a
validation set, or even better, \textit{k}-fold cross-validation, introduced in
Section \ref{sec:selectass} This work includes both the testing error (using
the testing set described in Section \ref{sec:training}) and cross-validation
error. The predetermined testing set will allow for comparison against the
previous work it was obtained from, but it is assumed that cross-validation
will provide a better indication of model performance.

The learning curves are obtained as follows, shown in Figure %\ref{fig:lc1}.
For a given (randomly chosen) training set size between 15 and 100\% of the
total data set, training and prediction rounds were performed for each. The
testing error scenario performs this \textit{k} times and averages those
results.  This is equivalent to the \textit{k} in \textit{k}-fold
cross-validation to provide some semblance of equivalent statistics.  The
cross-validation error scenario has no need for averaging because it is
performed automatically.  In both cases, the learning curves do not provide a
clear picture of over- or undertraining. This peculiar behavior is caused by
severely biased model, as the training error should never be higher than the
testing error.

The validation curves are obtained as follows, shown in Figure %\ref{fig:vc1}.
For a given range of some algorithm parameter influencing model complexity,
training and prediction rounds were performed for different values within the
range.  Again, the testing and cross-validation errors are both used as
described above. As with the above training curve, determining the robustness
to over- or undertraining is difficult here although there is some minima seen
at \todo{finish when plot is selected}

The resolution to the underfitting is discussed later in \ref{sec:newtrain}.

\subsection{Model Comparison}
\label{sec:algcompare}

Regression Training Error: Confidence intervals on predictions to understand
true error versus sample error Test set must be > 30 instances, Can easily
calculate N\% confidence interval.

Options for comparison of algorithms: inverse bayesian stuff, comparing
classification of 2 classes on same ROC plot with multiple ML systems, Scatter
plots, Pairwise t-tests.
\todo{Discuss understanding confidence intervals in predictions.}

Alg Compare: inverse prob theory
In addition to evaluating a single learned model, it may be beneficial to
compare models. As discussed in \todo{the algorithm comparison section}, there
are three methods that will be used: comparison of \gls{ROC} curves, scatter
plots, and pairwise \textit{t}-tests.


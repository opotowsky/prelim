\chapter{Introduction}
\label{ch:intro}

The realm of nuclear security involves many parallel efforts in
nonproliferation (verification of treaty compliance, monitoring for smuggling,
proper storage and transportation of nuclear materials), cyber security,
minimizing stocks of weaponizable materials, disaster response training, and
nuclear forensics. All of these efforts have been continually improving, but
there was a gap regarding the ability of the \gls{US} to coordinate and respond
to a nuclear incident, especially with the technical portion of nuclear
forensics: characterization and analysis. After all, the first textbook on the
topic was published in 2005 \cite{nftext_2005}. In 2006, the \gls{US} \gls{DHS}
founded the \gls{NTNFC} within the \gls{DNDO}. The mission of the \gls{NTNFC}
is to establish a robust nuclear forensics capability to attribute radioactive
materials with demonstrable proof.

There are many fields that contribute to the nuclear forensics capability, such
as radiochemical separations, material collection techniques, improving
detector technology, material library development, and identifying forensic
signatures. These needs vary based on whether the material being collected is
post-detonation (e.g., bomb debris) or pre-detonation (e.g., \gls{SNF}).  In
the pre-detonation realm, this project focuses on statistical methods to
identify correlated material characteristics, which can lead to new forensic
signatures. 


\section{Motivation}
\label{sec:motivation}

Nuclear forensics is an important aspect of deterring nuclear terrorism even
though it is not, at first glance, thought to be preventative nuclear security.
The most common defense of the field is that nuclear forensics capability
deters state actors, not terrorist organizations. While it is true that a
strong capability encourages governments to be more active in prevention of
nuclear terrorism, it can also deter the terrorist organizations as well by
increasing their chances of failure. Less destructive success tends to be more
valued than high-risk mass destruction. In addition to influencing governments
and making nuclear terrorism higher risk for organizations, nuclear forensics
can assist in cutting off certain suppliers of nuclear materials or
technologies (e.g., nuclear specialists that are only involved for financial
reasons, access to state suppliers).  Shutting off the sources builds a
concrete barrier to nuclear terrorism.  Therefore, nuclear forensics is
considered impede this form of terrorism in both tangible and abstract ways
\cite{aps_aaas_forensics}.

Following the prevention value of nuclear forensics, it is important to
understand the process of the technical portion of the investigation and how
that can be improved.  In the event of a nuclear incident, such as the
retrieval of stolen \gls{SNM} or the detonation of a dirty bomb, it is
necessary to learn as much as possible about the source of the materials in a
timely manner. In the case of non-detonated \gls{SNM}, knowing the reactor
parameters that produced it can point investigators in the right direction in
order to determine the chain of custody of the interdicted material.  Section
\ref{sec:nfneeds} covers the specific needs of the nuclear forensics community
for \gls{SNF} provenance, and Section \ref{sec:statscontrib} discusses how
alternative computational approaches are useful, with a focus on why
statistical methods in particular are being pursued. 

\subsection{Needs in Nuclear Forensics}
\input{chapters/intro/forensics}
\label{sec:nfneeds}

\subsection{Contribution of Statistical Methods}
\input{chapters/intro/statlearning}
\label{sec:statscontrib}

\section{Methodology}
\label{sec:methodology}

As previously mentioned, the typical workflow of the technical portion of a
forensics investigation is to take measurements of an unknown material and
compare those measurements to databases filled with previously measured
standard materials. As this work focuses on \gls{SNF}, these measurements are
elemental, chemical, and radiological in nature.  Because creating databases
from real measurements to represent \gls{SNF} from reactor technologies from
around the world is not within the scope of this project, the database in this
study will be created from high-fidelity simulations via \gls{ORIGEN}
\cite{origen} within the SCALE code system \cite{scale} for modeling and
simulation. 

Figure \ref{fig:compworkflow} compares the \gls{INDEPTH} and statistical
methodologies, both of which use simulated \gls{SNF}.  While not all steps are
required to be equivalent, the only difference here is the method one chooses
to obtain reactor parameters. Both workflows address speed of characterization,
as it is intended to have gamma spectra as the inputs.  Because \gls{INDEPTH}
is better studied and validated than statistical methods, \todo{cite} this work
focuses on a statistical approach but with the intention to compare
methodologies.  Both workflows also address many of the database issues,
described above. The bottom statistical methodology is described here.
\\
\begin{figure}[!tbh]
  \includegraphics[width=\linewidth]{./chapters/intro/CompStatForensicsWorkflow.png}
  \caption{Two forensics research workflows using computational methodologies}
  \label{fig:compworkflow}
\end{figure}

In the simulation and statistical learning paradigm, we need to determine how
much information to what quality is needed to train a machine-learned model;
the model must give appropriate predictions of reactor parameters given a set
of measurements from a test sample of interdicted \gls{SNF}. Thus, the space 
that the training set encompasses must be chosen carefully so as to represent 
a likely scenario for stolen \gls{SNF}.

The next step is to choose an algorithm that performs statistical learning.
Statistical learners have varied strengths and weaknesses based on what is
being predicted and how they implement optimization.  Chosen for this study are
simple regression algorithms for burnup prediction: nearest neighbor and ridge
regression.  For comparison, \gls{SVR} is used because it is known to handle
highly dimensional data sets well.  These algorithms are introduced in Section
\ref{sec:algs}.

After the training is complete, the results of each models' predictions must be
evaluated.  Typically, a test set is used to compare against the model created
from the training set.  The testing error can therefore be tabulated with
respect to various specifications such as the training set size, number of
features, or algorithm parameters. These results are broadly known as
diagnostic plots and show if the algorithms' predictions are due to good
performance or bad fitting. 

After the models are evaluated using machine learning best practices, it will
be important to compare them both against each other and against other
computational forensics parameter methods. Thus, a Bayesian approach from the
field of inverse problem theory will be used to give the probability density of
the predictions so that the statistically generated predictions can be
evaluated directly against other solutions, such as optimization-based methods
or direct computations. 

Next, information reduction (within the training and/or testing data sets) must
be used to investigate the extension of this workflow to the real world. The
primary example is the reduction of information quality via gamma ray
detectors, as they can provide fast results.  If an algorithm could overcome
the limitations of gamma detection and still provide useful results, this would
warrant further studies and perhaps be field-applicable.

Thus, ultimately, the goal is to answer the question \textit{How
does the ability to determine forensic-relevant spent nuclear fuel attributes
using machine learning techniques degrade as less information is available?}. 

\section{Goals}

The main purpose of this work is to evaluate the utility of statistical methods
as an approach to determine nuclear forensics-relevant quantities as less
information is available. Machine learning algorithms are used to train models
to provide these values (e.g., reactor type, time since irradiation, burnup)
from the available information. The training data is simulated using
\gls{ORIGEN}, which provides an array of nuclide concentrations as the features
($X$) and the parameters of interest ($y$) are provided from the simulation
inputs.  Information reduction is carried out using computationally generated
gamma spectra; the radionuclide concentrations from the simulations can be
converted into gamma energies, which then undergo a detector response
calculation to represent real as-measured gamma spectra as closely as possible.
Machine learning best practices are used to evaluate the performance of the
chosen algorithms, and inverse problem theory is used to provide an interval of
confidence in the model predictions.

The necessary background is covered in Chapter \ref{ch:litrev}.  First, an
introduction to the broader field of nuclear forensics is in Section
\ref{sec:nfoverview} to place this work in the context of the technical mission
areas. After that, a short discussion of the field of machine learning, the
algorithms used, and validation methods are in Section \ref{sec:mlback}.
Section \ref{sec:fcsim} includes information about the codes used to generate
the training data, via fuel cycle simulation, detector response function, and
isotope identification of gamma spectra.  Lastly, a review of statistical
methods being used in studies of forensics analysis is covered next in Section
\ref{sec:stats4nf}. 

After the existing work is discussed, the methodology and a demonstration of
the experimental components is introduced next in Chapter \ref{ch:demo_method}.
This will cover the simulated training data in Section \ref{sec:training}, the
parameters behind the learned models in Section \ref{sec:statmodel}, and the
process of model evaluation in Section \ref{sec:valid}.  

Finally, Chapter \ref{ch:proposal} summarizes the official thesis research
proposal. After the preparatory tasks are covered in Section \ref{sec:prep},
there are three experiments outlined in Sections \ref{sec:exp1},
\ref{sec:exp2}, and \ref{sec:exp3}. Qualitiative hypotheses as well as
alternative directions for risk mitgation are discussed throughout this
chapter as well. 

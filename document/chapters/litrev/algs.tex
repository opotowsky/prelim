\setlength\abovedisplayskip{2.5pt}

For relevant nuclear forensics predictions, both classification and regression
algorithms must be used.  For example, one may want to predict the reactor type
based on some measurements (referred to as features) of \gls{SNF} of an unknown
source, and this would require a classification algorithm. Or perhaps the input
fuel composition is relevant to an investigation on weapons intent, so a
regression algorithm would be used to train a model based on some set of
features.  Since this work trains models to predict burnup of \gls{SNF}, the
algorithms are presented in a regression context.

\subsubsection{Linear Models}
\label{sec:linear}

One of the simplest and most utilized methods of prediction is a linear model
from a least-squares fit. Thus, it is the most natural place to begin a
demonstration of machine learning algorithms. Since linear models must have all
parameters linearly related, there are many restrictions regarding the shape of
the model. However, this makes the resulting models stable to peturbations. 

\begin{figure}[!htb]
  \centering
  \includegraphics[width=\linewidth]{./chapters/litrev/regularization.png}
  \caption{Illustration of the effect of regularization on model prediction}
  \label{fig:reg}
\end{figure}

An example of a linear model is in the equation below, where the vector of
input features of size $p$, $\boldsymbol{X}$, provides a model,
$F(\boldsymbol{X})$ ($\boldsymbol{Y}$ is the vector of known
data)\todo{reword}, by determining the unknown coefficients, or weights,
$\beta_{j}$'s. The algorithm calculates these by minimizing the value of a loss
function over all the training data.  This is usually the least squares error
from minimizing the sum of squared errors, $\sum_{i=1}^{n} (y_i - f(x_i))^2$.
But it could instead be the least absolute deviations from minimizing the sum
of absolute error differences, $\sum_{i=1}^{n} |y_i - f(x_i)|$. These are
referred to as the $L_2$ and $L_1$ norms, respectively.  
\begin{equation}
  F(\boldsymbol{X}) = \beta_{0} +  \sum_{j=1}^{p} x_{j} \beta_{j}
\end{equation}

The form of linear regression used here is called ridge regression. This
algorithm performs optimization using the $L_2$ norm, and also uses the form of
the $L_2$ norm for \textit{regularization}. Regularization, sometimes called
\textit{shrinkage}, is a term introduced into the predicted model to prevent
overfitting. (It is used in many machine learning algorithms.) This works by
further reducing the weights, $\beta_j$, on the input features, $x_j$. This
shrinkage term also includes a complexity parameter, $\lambda$
\cite{elements_stats}.  Thus, the predicted linear model from ridge regression
is updated to the following equation.  A visualization of what this
regularization can accomplish is in Figure \ref{fig:reg}.
\begin{equation}
  F(\boldsymbol{X}) = \beta_{0} +  \sum_{j=1}^{p} x_{j} \beta_{j} + \lambda \sum_{j=1}^{p} \beta_{j}^2
\end{equation}

\subsubsection{Nearest Neighbor Methods}
\label{sec:neighbor}

Nearest neighbor regression is considered to be `model-free' in that it does
not actually generalize; it tracks the observations in the training set.
During prediction, the algorithm will calculate a value based on the instance
that is closest to the current test sample. Thus, there is not any learning,
but instead a direct comparison between an unknown sample and the space that
the training set populates. The predictions from nearest neighbors can be quite
accurate, but are highly unstable to peturbations \cite{elements_stats}.  

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.8\linewidth]{./chapters/litrev/nn-fig.png}
  \caption{Schematic demonstration of regression using \textit{k}-nearest neighbors}
  \label{fig:nn}
\end{figure}

An extension of nearest neighbor is \textit{k}-nearest neighbor regression.
The closest \textit{k} neighbors are averaged to provide an estimate of the
unknown sample as shown in the equation below.  Figure \ref{fig:nn} provides a
pictoral explanation of how this is done for a single prediction.  The equation
below shows how this algorithm does this to determine a predict a value, $Y$,
from the input features, $\boldsymbol{X}$, in the neighborhood, $N_k
(\boldsymbol{X})$ \cite{elements_stats}. 
\begin{equation}
  Y(\boldsymbol{X}) = \frac{1}{k} \sum_{x_i \in N_k(\boldsymbol{X})} y_i
\end{equation}

A measure of distance dictates what the neighborhood is. The metrics for
distance differ, but in this study, the Euclidian distance was used. Another
parameter is the population of the neighborhood, \textit{k}. In this initial
work, $k = 1$ is used. This can perform very well, but can also easily overfit
the data and thus not generalize well. 

\subsubsection{Support Vector Machines}
\label{sec:svm}

\Acrfull{SVR} is an extension of the popular classification algorithm, \acrfull{SVM}.
This algorithm was chosen because of its ability to handle highly dimensional
data well, which in this study is a maximum of approximately 300 features. 

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.8\linewidth]{./chapters/litrev/svm.png}
  \caption{Schematic demonstration of classification using SVM}
  \label{fig:svm}
\end{figure}

As seen in Figure \ref{fig:svm}\footnote{This schematic is based on the
tutorial on SVMs by Dr.\@ Saed Sayad at
http://www.saedsayad.com/support\_vector\_machine.htm}, the \gls{SVM} algorithm
separates two classes by determining an optimal hyperplane between them.  The
algorithm evaluates the quality of the line that separates the two classes by
maximizing the margin width, $m_w = \frac{2}{\lVert w \rVert}$.  The hyperplane
is defined in the equation below, where $\boldsymbol{w}$ is the vector that is
normal to the hyperplane.  
\begin{equation}
  \boldsymbol{w \cdot x} + b = 0
\end{equation}

Figure \ref{fig:svm} also shows a case of soft margins.  Some problems are not
linearly separable, and thus a penalty term, $\xi_{i}$, is introduced to allow
for some misclassifications.  The algorithm then simultaneously minimizes the
misclassifications while maximizing the margin. The objective function of the
algorithm below shows this as well (using quadratic programming). In the
equation below, $C$ is responsible for the margin width/misclassification
tradeoff.
\begin{equation}
\begin{split}
  min\ & \frac{1}{2} \lVert w \rVert ^{2} + C \sum_{i} \xi_i \\
  subject\ to:\ \ & y_i (w x_i + b) > 1 - \xi_i
\end{split}
\end{equation}

Figure \ref{fig:svr}\footnote{These schematics are based on a tutorial on SVR
by Dr.\@ Saed Sayad at
http://www.saedsayad.com/support\_vector\_machine\_reg.htm} demonstrates how
\gls{SVM} can be altered slightly from classification to nonlinear regression
with \gls{SVR}.  \Gls{SVR} has a similar objective function but instead
\textit{minimizes} the margin, as shown in Figure \ref{fig:svr-a}. This changes
the constraints to the objective function as below:
\begin{equation}
\begin{split}
  min\ & \frac{1}{2} \lVert w \rVert ^{2} + C \sum_{i} \xi_{i} \\
  subject\ to:\ \ & \lvert y_i - (w x_i + b) \rvert \leq \varepsilon + \xi_i
\end{split}
\end{equation} 

Further, this can be extended to nonlinear analysis via what is called the
\textit{kernel trick}.  First, using a nonlinear kernel function maps the data
into higher dimensional\todo{check wording} feature space. Then the algorithm can find a linear
separation in this space, as shown in Figure \ref{fig:svr-b}.

\begin{figure}[!hp]
  \centering
  \begin{subfigure}[h]{0.8\linewidth}
    \includegraphics[width=\linewidth]{./chapters/litrev/svr-a.png}
    \caption{Demonstration of regression with \gls{SVR}}
    \label{fig:svr-a}
  \end{subfigure}
  \begin{subfigure}[h]{0.8\linewidth}
    \includegraphics[width=\linewidth]{./chapters/litrev/svr-b.png}
    \caption{The kernel trick with \gls{SVR}}
    \label{fig:svr-b}
  \end{subfigure}
  \caption{Illustrations of how \gls{SVM} can be extended to both regression and nonlinear analysis}
  \label{fig:svr}
\end{figure}

The kernel chosen for this study is the Gaussian radial basis function, shown
below. This has two tuneable parameters, $\gamma$ and $C$. The $\gamma$
controls the width of influence of individual training instances, which
strongly affects the fitting of the model. Low values correspond to
underfitting because the instances have too large of a radius (low influence)
and high values correspond to overfitting because the instances have a small
radius (high influence). 

The $C$ parameter also affects the fitting of the model by allowing more or
less support vectors, corresponding to more or less misclassification,
respectivey. A lower $C$ smooths the surface of the model by allowing more
misclassifications, whereas a higher $C$ classifies more training examples by
allowing fewer misclassifications. Thus, too low or too high of a C can cause
under- or overfitting, respectively. 

Since there is a tradeoff of fitting strength provided by both parameters, it
is common to run the algorithm on a logarithmic grid from $10^{-3}$ to $10^3$
for each parameter. If plotted on a heatmap of accuracies given $\gamma$ and
$C$, there will be a diagonal of ideal combinations that emerges. The element
with the lowest of each parameter is usually chosen. 

\subsubsection{Dimensionality Reduction Techniques}
\label{sec:dimreduc}

In addition to utilizing various algorithm parameters for regularization as
discussed above, dimensionality reduction can improve generalizability by
removing the noise of features that do not affect the regression task. This can
be thought of in the following way: shrinkage techniques reduce the weights of
noisy features, whereas dimensionality reduction removes them completely.
Although one could use domain knowledge to manually reduce the number of
features in a data set (e.g., only including certain nuclide subsets such as
actinides), statistical feature reduction may also prove helpful in this work.
The mathematical treatment of the methods described below are in Ref.
\cite{elements_stats}.

%Another use for the following techniques is the visualization of one's data
%set, perhaps for data exploration or to show why certain predictions are
%difficult. 

\vspace{5mm} \noindent \textbf{Principal Components Analysis} \vspace{5mm}

\Gls{PCA} is considered the most common dimensionality reduction technique.
The \gls{PCA} algorithm learns a linear transformation of a data set
$\boldsymbol{X}$ with which to construct a transformation matrix according to a
user-chosen number of variables/components.  This matrix is part of the
singular value decomposition of the data matrix $\boldsymbol{X}$.  The
decomposition step is what provides the principal components, which are the
result of maximizing the variance in the original data while minimizing the
squared reconstruction error between the original data and the transformed
data. The mathematical assumptions are that the variables are all Gaussian and
uncorrelated.

Because the principal components are obtained purely statistically with no
model assumptions, they are usually uninterpretable. However, they can still
provide clues with which to obtain new information. In the case of this work,
this could provide insight into new forensics signatures.

\vspace{5mm} \noindent \textbf{Factor Analysis} \vspace{5mm}

Factor analysis is similar to \gls{PCA} in that it also calculates linear
combinations of the data set features using the above-mentioned decomposition
and the mathematical assumptions are the same.  It is different in that the
decomposition is rearranged so that it represents \textit{latent variables}
including random error disturbances rather than principal components.  Latent
variables are constructed by maximizing the correlation/shared variance among
the variables rather than the total variance.  However, different optimizations
can be chosen so that the solutions are parameter-dependent.  

Furthermore, the initial model assumptions, while enabling interpretable
results, also increase the dependence of the solutions on the algorithm inputs.
If \gls{PCA} does not perform well with the type of training data in this work,
it is possible that factor analysis will, since there are many nuclides in the
training data set that are in a decay chain together.

\vspace{5mm} \noindent \textbf{Independent Components Analysis} \vspace{5mm}

\Gls{ICA} has characteristics of both factor analysis and \gls{PCA}, but with
the goal of finding independent measurements from multiple `sources'.  It uses
the same form of decomposition as factor analysis but without the inclusion of
random error. The mathematical assumptions are a bit different: the variables
are statistically independent and non-Gaussian.  This allows the algorithm to
minimize higher-order statistics of the data set (\gls{PCA} and factor analysis
only minimize the first two orders).

Since the independent components are useful in signal processing of multiple
signals, this technique could prove useful for reprocessed nuclear materials
where multiple source streams converge. 


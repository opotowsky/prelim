Machine learning is a sub-field of \gls{AI} within the broad category of
computer science. The goal of \gls{AI} is to create computer systems that
respond to their environment according to some set of criteria or goal. For
example, self-driving vehicles have computers on board that learn to avoid
curbs and humans. It is common knowledge that the use of \gls{AI} has been
expanding at a rapid rate in recent years. News stories of major tech
companies' \gls{AI} advancements are frequent and news articles abound with
data on which jobs will be replaced with \gls{AI} in the near future. 

While its use has been increasing in the commercial sector, there is also much
anecdotal evidence to support the existence of a rapid increase of \gls{AI} use
in academic research across many disciplines beyond robotics. \gls{AI} systems
have been used in detection (e.g., fraud or spam), medical diagnostics, user
analytics, and a host of scientific disciplines that have increasing amounts of
multivariate data.

Much of the recent advances to the field of \gls{AI} have occured in the
statistical realm, which forgoes domain knowledge in favor of large data sets.
Thus, machine learning and statistical learning has become somewhat of a
separate field. \todo{could explain more, cite changing science of ML} Machine
learning research focuses on the underlying algorithms using mathematical
optimization, methods for pattern recognition, and computational statistics.
As an application, however, this study is not concerned with computational
time, but rather the ability to correctly predict values and categories
relevant to the nuclear forensics mission. However, differences among the
algorithms do impact the performance of a machine-learned model. Therefore the
algorithms used in this study will be discussed.  

Machine learning tools or no? (supervised learning, anomaly detection,
clustering, ANNs, dim reduction)

\subsection{Classification and Regression Algorithms}
\label{sec:algs}

Blah blah I know it's only regression right now

\subsubsection{Nearest Neighbor Regression}
\label{sec:nnreg}

\todo{intro, alg math, explain distance metrics} 

Nearest neighbor regression calculates a value based on the instance that is
closest to it. The metrics for distance differ, but in this study, Euclidian
distance was used. There is no learning in this regression, per se; the
training set populates a space and the testing set is compared directly to
that. 

\subsubsection{Ridge Regression}
\label{sec:rreg}

\todo{intro, alg math, explain regularization} 

Ridge regression utilizes normal linear least squares regression, but with a
parameter that penalizes correct answers to prevent overfitting, which is
referred to as regularization. 

\subsubsection{Support Vector Regression}
\label{sec:svr}

\todo{add schematics from pres...or diff pics with the math?} 

Support vector regression (SVR) is an extension of the popular classification
algorithm, support vector machine (SVM).  This algorithm was chosen because of
its ability to handle highly dimensional data well, which in this study is
approximately 300 features. 

SVM classifies two classes by determining an optimal hyperplane, given by wx+b,
between them.  As seen in Figure ?, the algorithm evaluates the quality of the
line that separates two classes by maximizing the width of the margin given the
contraints surrounding the line.  Some problems are not linearly separable, and
thus a penalty term is introduced to allow for misclassifications. As shown in
Figure ?, the algorithm then simultaneously minimizes the misclassifications
while maximizing the margin. 

This can be extended easily to multidimensional analysis via what is called the
\textit{kernel trick}.  First, using a nonlinear kernel function maps the data
into higher dimensional feature space. Then the algorithm can find a linear
separation in this space, as shown in Figure ?. Further, this can be upgraded
from classification to SVR by doing similar math but instead minimizing the
margin, as shown in Figure ?. 

The kernel chosen for this study is the Gaussian radial basis function, shown
below. This has two tuneable parameters, gamma and C. Gamma influences the
width of influence of individual training instances, and strongly affects the
fitting of the model. Low values correspond to underfitting because the
instances have too large of a radius (low influence) and high values correspond
to overfitting because the instances have a small radius (high influence). 

The C parameter also affects the fitting of the model by allowing more or less
support vectors, corresponding to more or less misclassification. A lower C
smooths the surface of the model by allowing more misclassifications, whereas a
higher C classifies more training examples by allowing fewer
misclassifications. Too low and too high of a C can cause under- and
overfitting, respoectively. 

Since there is a tradeoff of fitting strength provided by both parameters, it
is common to run the algorithm on a logarithmic grid from 10'-3 to 10'3 for
each parameter. If plotted on a heatmap of accuracies given gamma and C, there
will be a diagonal of ideal combinations that emerges. The lowest of these is
usually chosen. 

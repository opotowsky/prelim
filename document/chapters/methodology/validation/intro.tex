As previously mentioned in the \todo{add label} demonstration section, the
validation was carried out as a comparison of accuracies of the predicted
classes (reactor type, \todo{other classification tasks?}) and values
(enrichment amount \todo{or burnup?}) to the classes and values in the
predetermined test data set (discussed in \todo{add label when this section is
done}). 

However, because it is difficult to ensure consistently representative testing
data, the accuracy of a learned model should not depend on only one testing set
\todo{general knowledge or citation?}. The learned model's accuracy can be
better evaluated as discussed in \todo{the algorithm validation section} by
using cross validation \todo{should this be introduced in the background?}.
Other additional evaluation methods will be discussed here as well.  \todo{ROC
curves, confidence intervals on error, confusion matrix if necessary}

In addition to evaluating a single learned model, it may be beneficial to
compare models. As discussed in \todo{the algorithm comparison section}, there
are three methods that will be used: comparison of \gls{ROC} curves, scatter
plots, and pairwise \textit{t}-tests. 

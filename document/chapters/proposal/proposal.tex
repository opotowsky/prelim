\chapter{Research Proposal}
\label{ch:proposal}

This document previously demonstrated the performance of machine learning on a
set of nuclear material isotopics to calculate a reactor parameter of interest:
burnup.  Additionally, there have been various methods discussed for
understanding the learned model's behavior and thus the quality of the results.
Moving forward to a set of experiments is now possible.

Before describing the experiments, some topics and issues are addressed in
Section \ref{sec:prep}.  Finally, the proposed research experimental design is
presented Sections \ref{sec:exp1}, \ref{sec:exp2}, and \ref{sec:exp3}.

\section{Experiment Preparations}
\label{sec:prep}

\subsection*{Expanding Training Set}

As identified in Section \ref{sec:valid}, the testing set used for the
demonstration was not suitable for further study without being expanded.  Many
algorithms are developed on an assumption that the training set will be
\gls{i.i.d.}.  This is important so that the model does not overvalue or
overfit a certain area in the training space.  The next step is to provide a
larger, more diverse training set to the algorithms so they can better predict
when faced with new instances. This diversity will be suggested from the
\gls{SFCOMPO} database \cite{sfcompo}, as it includes many common domestic and 
international reactors.

The SFCOMPO-2.0 relational database \cite{sfcompo} has approximately 750
\gls{SNF} measurements from 44 reactors. While this is not sufficient as a
training set, it provides a better framework for simulating a larger training
set using \gls{ORIGEN}.  After cross-validation, diagnostics, and optimization,
the trained models can be tested against the entries in this database to
provide a clear estimate of of the model performance. 

\subsection*{Finalizing Set of Algorithms}

The three algorithms in the demonstration (nearest neighbor, ridge, and support
vector regression) are not necessarily the set that will be evaluated for the
experiments. After these are used to train new models on a larger training set
for comparison, other algorithms can be speedily assessed as well.  Since
support vector-, distance-, and linear-based models are already represented,
other obvious choices include Bayesian methods, decision trees, neural
networks, or ensemble methods \cite{elements_stats}. 

\subsection*{Computational Framework and Resources}

Thus far, all simulations and training have not required more processing than
available on a personal laptop. However, some algorithms do require larger
amounts of computational time (e.g., artifical neural networks).  If necessary,
the training stage can be done using the \gls{CHTC}, which is available to
University of Wisconsin researchers. 

\section[Experiment 1: Direct Isotopics]{Experiment 1\\ 
\large{\textit{Viability of Statistical Learning on Direct Isotopics}}}
\label{sec:exp1}

The first experiment will be a purposefully constructed version of the
demonstration: evaluating the model performance with known isotopics.  This
sheds light on how this methodology will perform on the simplest scenario,
providing an estimate on the maximum level of performance.  \textit{The main
purpose of this experiment is to iteratively probe the usefulness of
statistical methods for determining reactor parameters, ultimately choosing the 
best performing methods.}
\\
\begin{figure}[!htb]
    \centering
    \includegraphics[width=\linewidth]{./chapters/proposal/proposal.png}
    \caption{Physical and Computational Comparisons for Experiments 1 and 2}
    \label{fig:proposal}
\end{figure}

Figure \ref{fig:proposal} shows what type of input can be used for training and
testing (or prediction). The two horizontal boxes are the physical and
computational forms of what these experiments are simulating, respectively. The
lab-measured mass spectra correspond to the perfect information being referred
to here. In the computational context, these measurements instead come from
simulations. Mass spectra results are thus approximated as the direct isotopics
given from the simulations, since mass spectrometry provides highly reliable
and accurate information.

The variables for this experiment will include the following:
\begin{enumerate}
  \itemsep-0.75em
  \item the complexity of the machine learning algorithm used, 
  \item feature reduction, and 
  \item different subsets of the desicion space. 
\end{enumerate}
As previously mentioned, there are complexity differences among the algorithms,
which will be taken into account.  The distance-based nearest neighbor
algorithm is by far the simplest, as it does not do any learning. By contrast,
\gls{SVR} with a radial basis function kernel to work in many dimensions is
much more elaborate.  The feature reduction will be different subsets of
isotopes, e.g., top \textit{n} nuclides, fission products only, etc.  Reducing
the decision space can be done by, e.g., simplifying the regression task by
fixing the reactor type. It is expected that a more complex algorithm (e.g.,
\gls{SVR}) will be needed, and that preprocessing and/or manual feature
reduction will assist in creating higher quality models.  Simplifying the
decision space should always improve prediction, but it is not obvious how much
it will be needed for burnup prediction specifically.

\subsection*{Risks}

It is possible that statistical models trained on direct isotopic information
do not perform well enough.  Other than attempting different types of
algorithms, it is possible to preprocess the data, statistically performing
feature reduction via \gls{PCA}. If this is not sufficient, it is possible
\gls{SNF} is has too many or too few correlated features to provide reliable
models across the space of current reactor technologies. Since separated
plutonium and \gls{UOC} have been also studied using these techniques, it is
possible these materials can provide useable learned models. Additionally, this
methodology would also work if applied to post-detonation materials. There is
work on creating standard materials to represent the ``urban canyon'', so this
is another subject that could benefit from statistical correlations
\cite{refmaterial}.

\section[Experiment 2: Gamma Spectra]{Experiment 2\\ 
\large{\textit{Viability of Statistical Learning on Gamma Spectra}}}
\label{sec:exp2}

The second experiment will be the previously discussed extension of the
demonstration by applying detector response functions to the \gls{SNF}
isotopics: evaluating the model performance with reduced isotopic information.
This demonstrates the usefulness of this methodology in a real-world scenario
where exact isotopics are not always known.   \textit{The main objective of
this experiment is to measure the reduction in statistical model parameter
prediction reliability as the quality of the training information is reduced.}

The two bottom portions of the boxes in Figure \ref{fig:proposal} represent a
more realistic measurement scheme, involving a model trained from gamma
spectrometers rather than the lengthy process of performing mass spectrometry
on the samples.  In the physical context, the measurements for training would
be done using a semiconductor gamma detector, but the testing or prediction
step may be done outside of the lab on a different detector.  This will be
captured by applying different detector response functions to the radionuclide
inventories from the simulations.

The variables for this experiment will include the following:
\begin{enumerate}
  \itemsep-0.75em
  \item the complexity of the machine learning algorithm used, 
  \item feature reduction (implicit), and 
  \item quality of training and/or testing data set.
\end{enumerate}
As in Section \ref{sec:exp1}, the algorithm complexity will be used to explain
performance.  The feature reduction is implicit here, since gamma detection
only includes radionuclides within the \gls{SNF} isotopics. The indirect
isotopic training data are likely going to reduce the prediction capability of
the models, but it is not yet clear if a response function simulating a
hand-held NaI gamma detector can provide any useful predictions.  And while it
is still expected that the complex algorithms will perform better, it is not
yet obvious if different algorithms than the ones used in Experiment 1 will be
needed. 

\subsection*{Risks}

It is possible that statistical models trained on indirect isotopic information
do not perform well enough. Again, here, different algorithms may perform
better than others due to the underlying optimization processes. Further
feature reduction could also prove useful, focusing on particular energy
regions or particular peaks throughout the spectrum. The quality of the
training information can be improved slightly by using an isotope
identification algorithm; this may improve the performance, as they are
developed to automatically report isotopics from gamma spectra. If this still
is not sufficient, it may be that direct isotopic information (i.e., that
obtained from mass spectrometry) is required for reliable statistical models of
\gls{SNF}.  Although preprocessing could also be investigated here, the
materials discussed above may also be more disposed to defined statistical
correlations.

\section[Experiment 3: Other Fuel Cycle Flows]{Experiment 3\\ 
\large{\textit{Viability of Statistical Learning on Other Fuel Cycle Flows}}}
\label{sec:exp3}

There is already nuclear fuel reprocessing in France for light water reactors,
and possibly in the forseeable future in China with fast breeder reactors.
Given the inevitablity of reprocessing in fuel cycles, it is important to
develop nuclear forensics capabilities on processed \gls{SNF}. This is presents
an additional prediction challenge because many of the isotopic, chemical, and
elemental signatures are stripped away in the processing. However, there is a
possibility that plutonium isotopes can provide enough information for
characterization, even with reprocessing \cite{pu_discrimination}. Although
Ref. \cite{pu_discrimination} predicts reactor type and enrichment, it is
enough motivation to pursue burnup as well. 

This experiment is intended to repeat those in Sections \ref{sec:exp1} and
\ref{sec:exp2} with an added layer of complexity from the reprocessing.  It
will be carried out using direct isotopics as in Section \ref{sec:exp1}, with a
goal to use indirect information as in Section \ref{sec:exp2} pending
performance. The training data set will also be simulated using \gls{ORIGEN}.
\textit{The main purpose of this experiment is to probe the difficulties of
reactor parameter prediction when some of the inputs are from a cyclical
nuclear fuel cycle.}

The variables for this experiment will include the following:
\begin{enumerate}
  \itemsep-0.75em
  \item the complexity of the machine learning algorithm used,
  \item quality of training data set, and 
  \item type of preprocessing for feature reduction.
\end{enumerate}
Because of the increased complexity of the training data, it is expected that a
more involved algorithm will be necessary.  The quality of the training data
set will be varied as well as the type of preprocessing tool.  It is not yet
obvious if the feature reduction is absolutely necessary, but it is expected to
drastically improve the accuracy of burnup prediction.  Feature reduction can
be carried out via a range of discriminant, component, or factor analyses.
Multiple studies have demonstrated success with some of these dimensionality
reduction techniques \cite{nicolaou_2006, nicolaou_2009, nicolaou_2014,
robel_2009, pu_discrimination, jones_viz_2014, jones_snf_2014}. Because
reprocessing mixes streams of material, \gls{PCA} is not expected to perform as
well as \gls{ICA}.

\subsection*{Risks}

It is possible that this framework is not the best approach to understand fuel
cycles with reprocessing. The first action upon poor performance results is to
try more/different preprocessing tools; this may have to include using domain
knowledge to manually reduce the dimensions. As one of the goals within the
nuclear forensics community is to identify new signatures and/or correlated
measurements, this is not desireable but it could still offer interesting
results. Also, some creativity could be applied to find a method that works for
this type of dataset, such as the iterative partial least squares discriminant
analysis used for \gls{UOC} country and source prediction in Refs.
\cite{robel_2009, pu_discrimination}.
